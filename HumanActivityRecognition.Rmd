---
title: "Human Activity Recognition"
author: "Alex M."
date: "2/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

The goal of this project is to predict the manner in which users of devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* did exercise.

The data collected from accelerometers on the belt, forearm, arm, and dumbell of  6 participants, who perform barbell lifts correctly and incorrectly, allow us to analyze how well they did the exercise.


## Loading the data

First, we are going to load the data:

* The training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
We are going to use this training data set for fitting our models and choose the best algorithm.
  
* The test data : https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

We are going to use this data set for making a prediction based on the results of chosen model. 
```{r load data, echo=TRUE}
library(dplyr)
library(corrplot)
library(caret)
library(randomForest)
library(naniar)
set.seed(123)

#LOADING DATA
URLtrain<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
URLtest<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training<-read.csv(URLtrain)
testing<-read.csv(URLtest)

dim(training)
dim(testing)
```

As we can see we have a training data set with 19622 observations and 160 variables(including the class), and a testing data set of 20 observations and 160 variables, but now, we observe we don't have the class variable instead we have an identification variable (problem_id), so we are going to predict the class.

## Partitioning the data

Now we are going to separate the training data set in two parts, one with the 70% of the data to fit our model, and the rest (30%) to evaluate the performance of the model.
```{r data partition, echo=TRUE}

#dataset partition
dataindex <- createDataPartition(training$classe, p=0.7, list=FALSE)
traindata <- training[dataindex, ]
testdata <- training[-dataindex, ]

```

## Basic Exploratory Data Analysis

Next, we are going to do some basic explorations in the data sets
```{r exploratory analysis, echo=TRUE }
#Data sets dimensions
dim(traindata)
dim(testdata)

#Let's look at the values of class variable that indicates how well the exercise is done
unique(traindata$classe)

#look at data set structure
str(traindata)
```
As we can see, there are many variables with NAs and empty values.

### NAs and near zero variance Analysis 

First, we are going to analyze the data with near zero variance...
```{r NZV, echo=TRUE }
#exclude the near zero variance
NZV<-nearZeroVar(traindata)
length(NZV)
```
As we can see, we have a lot of variables with variance near zero. This variables represent noisy information, so we can remove it from the train and test data sets...
```{r NZV exclude, echo=TRUE }
#exclude the near zero variance
traindata2<-traindata[,-NZV]
testdata2<-testdata[,-NZV]
dim(traindata2)
dim(testdata2)
```
Now we are going to analyze the variables with most part of the data are NAs...
```{r NAs, echo=TRUE }
#we have to analyze NAs and remove the ones with mostly NA
MostlyNA<-apply(is.na(traindata2), 2, mean)>0.95
sum(MostlyNA)
```
Like we did with previously, we can remove this variables from our data sets...
```{r NAs exclude, echo=TRUE }
#removing mostly NAs variables
traindata3<-traindata2[, MostlyNA==FALSE]
testdata3<-testdata2[,MostlyNA==FALSE]
dim(traindata3)
dim(testdata3)
```
Finally, we can exclude the first five variables (X, user_name...) from our data set, because they don't bring useful information for fitting our model...
```{r excluding vars, echo=TRUE }
#removing the first five vars

#removing from training data set
traindata4<-select(traindata3,-X,-user_name,-raw_timestamp_part_1,-raw_timestamp_part_2,-cvtd_timestamp)

#removing from test data set
testdata4<-select(testdata3,-X,-user_name,-raw_timestamp_part_1,-raw_timestamp_part_2,-cvtd_timestamp)

#final dimensions
dim(traindata4)
dim(testdata4)

#Summary with missing values
pct_miss(traindata4) 
pct_miss(testdata4) 
```
Finally we have a data sets with 54 variables, and the percentage of missing value in data is zero.

For further analysis we can make a study of how highly correlated the variables are for making other data sets for better predictions.
```{r correlation analysis, echo=TRUE }
corrplot(cor(select(traindata4,-classe)), order = "FPC",type = "upper",tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```

## Machine Learning Algorythms

Next, we are going to fit two models with two different machine learning algorithms:

* Random Forest
* Generalized Boosted

In both cases we are using cross-validation methods.

Finally, we will choose the algorithm with better performance and we will use it to predict the class in the testing data set.

### Random Forest

First, we are going to fit a model using the Random Forest with cross-validation algorithm... 
```{r RF model, echo=TRUE, cache=TRUE }
#Converting the class variable to factor
traindata4$classe<-factor(traindata4$classe)
testdata4$classe<-factor(testdata4$classe)

#RANDOMFOREST + CV
set.seed(123)
kFolds <- trainControl(method = "cv", number = 10)
modFitRF<- train(classe ~ ., data=traindata4, method="rf",trControl=kFolds)
modFitRF
```
We observe that we obtain a very high accuracy and Kappa values, and we get the maximum  accuracy when the number of variables randomly sampled as candidates at each split is 27...
```{r RF plot, echo=TRUE, cache=TRUE }
plot(modFitRF)
```

Next, we are going to make a prediction with the test data set partition...
```{r RF prediction , echo=TRUE }
#prediction on Test dataset
predictRF <- predict(modFitRF, newdata=testdata4)

#Confusion Matrix
confMatRF <- confusionMatrix(predictRF, testdata4$classe)
confMatRF
```
As we can see in the confusion matrix, we have only 12 observations incorrectly classified, so we have a very high accuracy and a Kappa value near 1.

### Generalized Boosted

Now, we are going to fit anothe model using the Generalized Boosted with cross-validation algorithm 
```{r GB model, echo=TRUE, cache=TRUE  }
#PREDICTION GENERALIZED BOOSTED with CV
set.seed(123)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
modFitGBM  <- train(classe ~ ., data=traindata4, method = "gbm",trControl = controlGBM, verbose = FALSE)
modFitGBM
```
We observe that we obtain a very high accuracy and Kappa values with 150 trees and 3 iterations in depth....
```{r GB plot, echo=TRUE, cache=TRUE }
plot(modFitGBM)
```


```{r GB prediction, echo=TRUE }
# prediction on Test dataset
predictGBM <- predict(modFitGBM, newdata=testdata4)
confMatGBM <- confusionMatrix(predictGBM, testdata4$classe)
confMatGBM
```
As we can see in the confusion matrix, we have 59 observations incorrectly classified,a high accuracy and a Kappa value near 1.

## Final Conclusions

Now we can compare the accuracy and kappa values we have obtained in each model. The kappa coefficient measures the agreement between classification and truth values. So a kappa value near of 1 (or 100%) represents perfect agreement.

First we observe that the Random Forest model has a very high accuracy and kappa values...
```{r RF Accury plot, echo=TRUE }
plot(confMatRF$table, col = confMatRF$byClass, main = paste("Random Forest - Accuracy =",round(confMatRF$overall['Accuracy']*100, 2),"%"))

plot(confMatRF$table, col = confMatRF$byClass, main = paste("Random Forest - Kappa =",round(confMatRF$overall['Kappa']*100, 2),"%"))
```

Lets see now the values obtained with the Generalized Boosted model...
```{r GB Accury plot, echo=TRUE }
plot(confMatGBM$table, col = confMatGBM$byClass, main = paste("Generalized Boosted - Accuracy =",round(confMatGBM$overall['Accuracy']*100, 2),"%"))

plot(confMatGBM$table, col = confMatGBM$byClass, main = paste("Generalized Boosted - Kappa =",round(confMatGBM$overall['Kappa']*100, 2),"%"))
```

We can conclude that the Random Forest model has a better performance than the Generalized Boosted model with higher accuracy and Kappa values.  

## Final Prediction

Finally we are going to predict the class of the testing data set using the Random Forest fitted model
```{r Final testing prediction, echo=TRUE }
predictTesting <- predict(modFitRF, newdata=testing)
predictTesting
```